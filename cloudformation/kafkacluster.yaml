---
AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS CloudFormation Template to create a confluent Kafka Cluster'
Parameters:
  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instance
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: must be the name of an existing EC2 KeyPair.
  InstanceType:
    Description: WebServer EC2 instance type
    Type: String
    Default: t2.medium
    AllowedValues: [t1.micro, t2.nano, t2.micro, t2.small, t2.medium, t2.large, m1.small,
                        m1.medium, m1.large, m1.xlarge, m2.xlarge, m2.2xlarge, m2.4xlarge, m3.medium,
                        m3.large, m3.xlarge, m3.2xlarge, m4.large, m4.xlarge, m4.2xlarge, m4.4xlarge,
                        m4.10xlarge, c1.medium, c1.xlarge, c3.large, c3.xlarge, c3.2xlarge, c3.4xlarge,
                        c3.8xlarge, c4.large, c4.xlarge, c4.2xlarge, c4.4xlarge, c4.8xlarge, g2.2xlarge,
                        g2.8xlarge, r3.large, r3.xlarge, r3.2xlarge, r3.4xlarge, r3.8xlarge, i2.xlarge,
                        i2.2xlarge, i2.4xlarge, i2.8xlarge, d2.xlarge, d2.2xlarge, d2.4xlarge, d2.8xlarge,
                        hi1.4xlarge, hs1.8xlarge, cr1.8xlarge, cc2.8xlarge, cg1.4xlarge]
    ConstraintDescription: must be a valid EC2 instance type.
  SSHLocation:
    Description: The IP address range that can be used to SSH to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: "(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})"
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.
Mappings:
  AWSInstanceType2Arch:
    t1.micro:
      Arch: PV64
    t2.nano:
      Arch: HVM64
    t2.micro:
      Arch: HVM64
    t2.small:
      Arch: HVM64
    t2.medium:
      Arch: HVM64
    t2.large:
      Arch: HVM64
    m1.small:
      Arch: PV64
    m1.medium:
      Arch: PV64
    m1.large:
      Arch: PV64
    m1.xlarge:
      Arch: PV64
    m2.xlarge:
      Arch: PV64
    m2.2xlarge:
      Arch: PV64
    m2.4xlarge:
      Arch: PV64
    m3.medium:
      Arch: HVM64
    m3.large:
      Arch: HVM64
    m3.xlarge:
      Arch: HVM64
    m3.2xlarge:
      Arch: HVM64
    m4.large:
      Arch: HVM64
    m4.xlarge:
      Arch: HVM64
    m4.2xlarge:
      Arch: HVM64
    m4.4xlarge:
      Arch: HVM64
    m4.10xlarge:
      Arch: HVM64
    c1.medium:
      Arch: PV64
    c1.xlarge:
      Arch: PV64
    c3.large:
      Arch: HVM64
    c3.xlarge:
      Arch: HVM64
    c3.2xlarge:
      Arch: HVM64
    c3.4xlarge:
      Arch: HVM64
    c3.8xlarge:
      Arch: HVM64
    c4.large:
      Arch: HVM64
    c4.xlarge:
      Arch: HVM64
    c4.2xlarge:
      Arch: HVM64
    c4.4xlarge:
      Arch: HVM64
    c4.8xlarge:
      Arch: HVM64
    g2.2xlarge:
      Arch: HVMG2
    g2.8xlarge:
      Arch: HVMG2
    r3.large:
      Arch: HVM64
    r3.xlarge:
      Arch: HVM64
    r3.2xlarge:
      Arch: HVM64
    r3.4xlarge:
      Arch: HVM64
    r3.8xlarge:
      Arch: HVM64
    i2.xlarge:
      Arch: HVM64
    i2.2xlarge:
      Arch: HVM64
    i2.4xlarge:
      Arch: HVM64
    i2.8xlarge:
      Arch: HVM64
    d2.xlarge:
      Arch: HVM64
    d2.2xlarge:
      Arch: HVM64
    d2.4xlarge:
      Arch: HVM64
    d2.8xlarge:
      Arch: HVM64
    hi1.4xlarge:
      Arch: HVM64
    hs1.8xlarge:
      Arch: HVM64
    cr1.8xlarge:
      Arch: HVM64
    cc2.8xlarge:
      Arch: HVM64
  AWSInstanceType2NATArch:
    t1.micro:
      Arch: NATPV64
    t2.nano:
      Arch: NATHVM64
    t2.micro:
      Arch: NATHVM64
    t2.small:
      Arch: NATHVM64
    t2.medium:
      Arch: NATHVM64
    t2.large:
      Arch: NATHVM64
    m1.small:
      Arch: NATPV64
    m1.medium:
      Arch: NATPV64
    m1.large:
      Arch: NATPV64
    m1.xlarge:
      Arch: NATPV64
    m2.xlarge:
      Arch: NATPV64
    m2.2xlarge:
      Arch: NATPV64
    m2.4xlarge:
      Arch: NATPV64
    m3.medium:
      Arch: NATHVM64
    m3.large:
      Arch: NATHVM64
    m3.xlarge:
      Arch: NATHVM64
    m3.2xlarge:
      Arch: NATHVM64
    m4.large:
      Arch: NATHVM64
    m4.xlarge:
      Arch: NATHVM64
    m4.2xlarge:
      Arch: NATHVM64
    m4.4xlarge:
      Arch: NATHVM64
    m4.10xlarge:
      Arch: NATHVM64
    c1.medium:
      Arch: NATPV64
    c1.xlarge:
      Arch: NATPV64
    c3.large:
      Arch: NATHVM64
    c3.xlarge:
      Arch: NATHVM64
    c3.2xlarge:
      Arch: NATHVM64
    c3.4xlarge:
      Arch: NATHVM64
    c3.8xlarge:
      Arch: NATHVM64
    c4.large:
      Arch: NATHVM64
    c4.xlarge:
      Arch: NATHVM64
    c4.2xlarge:
      Arch: NATHVM64
    c4.4xlarge:
      Arch: NATHVM64
    c4.8xlarge:
      Arch: NATHVM64
    g2.2xlarge:
      Arch: NATHVMG2
    g2.8xlarge:
      Arch: NATHVMG2
    r3.large:
      Arch: NATHVM64
    r3.xlarge:
      Arch: NATHVM64
    r3.2xlarge:
      Arch: NATHVM64
    r3.4xlarge:
      Arch: NATHVM64
    r3.8xlarge:
      Arch: NATHVM64
    i2.xlarge:
      Arch: NATHVM64
    i2.2xlarge:
      Arch: NATHVM64
    i2.4xlarge:
      Arch: NATHVM64
    i2.8xlarge:
      Arch: NATHVM64
    d2.xlarge:
      Arch: NATHVM64
    d2.2xlarge:
      Arch: NATHVM64
    d2.4xlarge:
      Arch: NATHVM64
    d2.8xlarge:
      Arch: NATHVM64
    hi1.4xlarge:
      Arch: NATHVM64
    hs1.8xlarge:
      Arch: NATHVM64
    cr1.8xlarge:
      Arch: NATHVM64
    cc2.8xlarge:
      Arch: NATHVM64
  AWSRegionArch2AMI:
    us-east-1:
      PV64: ami-2a69aa47
      HVM64: ami-97785bed
      HVMG2: ami-0a6e3770
    us-west-2:
      PV64: ami-7f77b31f
      HVM64: ami-f2d3638a
      HVMG2: ami-ee15a196
    us-west-1:
      PV64: ami-a2490dc2
      HVM64: ami-824c4ee2
      HVMG2: ami-0da4a46d
    eu-west-1:
      PV64: ami-4cdd453f
      HVM64: ami-e722679e
      HVMG2: ami-af8013d6
    eu-west-2:
      PV64: NOT_SUPPORTED
      HVM64: ami-403e2524
      HVMG2: NOT_SUPPORTED
    eu-west-3:
      PV64: NOT_SUPPORTED
      HVM64: ami-8ee056f3
      HVMG2: NOT_SUPPORTED
    eu-central-1:
      PV64: ami-6527cf0a
      HVM64: ami-5652ce39
      HVMG2: ami-1d58ca72
    ap-northeast-1:
      PV64: ami-3e42b65f
      HVM64: ami-ceafcba8
      HVMG2: ami-edfd658b
    ap-northeast-2:
      PV64: NOT_SUPPORTED
      HVM64: ami-863090e8
      HVMG2: NOT_SUPPORTED
    ap-northeast-3:
      PV64: NOT_SUPPORTED
      HVM64: ami-83444afe
      HVMG2: NOT_SUPPORTED
    ap-southeast-1:
      PV64: ami-df9e4cbc
      HVM64: ami-68097514
      HVMG2: ami-c06013bc
    ap-southeast-2:
      PV64: ami-63351d00
      HVM64: ami-942dd1f6
      HVMG2: ami-85ef12e7
    ap-south-1:
      PV64: NOT_SUPPORTED
      HVM64: ami-531a4c3c
      HVMG2: ami-411e492e
    us-east-2:
      PV64: NOT_SUPPORTED
      HVM64: ami-f63b1193
      HVMG2: NOT_SUPPORTED
    ca-central-1:
      PV64: NOT_SUPPORTED
      HVM64: ami-a954d1cd
      HVMG2: NOT_SUPPORTED
    sa-east-1:
      PV64: ami-1ad34676
      HVM64: ami-84175ae8
      HVMG2: NOT_SUPPORTED
    cn-north-1:
      PV64: ami-77559f1a
      HVM64: ami-cb19c4a6
      HVMG2: NOT_SUPPORTED
    cn-northwest-1:
      PV64: ami-80707be2
      HVM64: ami-3e60745c
      HVMG2: NOT_SUPPORTED
Resources:

  KafkaMgmtRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: KafkaMgmt
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - 'ec2.amazonaws.com'
          Action:
            - 'sts:AssumeRole'
      Policies:
      - PolicyName: 'KafkaManagement'
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
              - 'ec2:*'
            Resource:
              - '*'

  KafkaInstanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: "/"
      Roles:
        - Ref: KafkaMgmtRole

  KafkaLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      InstanceType:
        Ref: InstanceType
      SecurityGroups:
      - Ref: KafkaInstanceSecurityGroup
      - 'kafka-servers-sg'
      KeyName:
        Ref: KeyName
      IamInstanceProfile:
        Ref: KafkaInstanceProfile

      UserData:
        'Fn::Base64': !Sub |
          #!/bin/bash -x
          export AWS_DEFAULT_REGION=eu-west-1

          # Make ec2-user's home dir the install to make it easier to navigate when subsequently ssh-ing in to instances
          su ec2-user -
          cd /home/ec2-user

          echo Working dir is `pwd`

          # Use the instance metadata to find out about the network the instance has been lauched into (the IP is the standard AWS metadata IP)
          AWS_METADATA_IP=169.254.169.254
          SELF_INSTANCE_ID=`curl http://$AWS_METADATA_IP/latest/meta-data/instance-id`
          #EC2_AVAIL_ZONE=`curl http://$AWS_METADATA_IP/latest/meta-data/placement/availability-zone`

          #S3_CONFIG_BUCKET=com.example.event-servers

          #ENI_NAME=event-servers-eni
          #ENI_INDEX=1


          #ZK_VERSION=zookeeper-3.4.9
          #ZK_CONFIG_FILE=zookeeper.config

          #KAFKA_VERSION=kafka_2.11-0.10.0.1
          #KAFKA_CONFIG_FILE=kafka.config

          attach_eni() {

            # find a free correctly tagged ENI in this AZ and attach it

            EC2_AVAIL_ZONE=`curl http://$AWS_METADATA_IP/latest/meta-data/placement/availability-zone`
            echo Running is AZ $EC2_AVAIL_ZONE

            # find a kafka ENI in the AZ we are running in that is not already attached -- we select the first 1
            # TODO we may need to do something here to avoid race conditions

            # we need to wait until an ENI is availble so if we don't get anything back then wait
            attached=255
            while [ "$attached" -gt "0" ]; do
              sleep 10
              echo waiting for ENI
              ENI_ID=`aws ec2 describe-network-interfaces --filters Name=tag-key,Values="kafkabrokerid" Name=tag-value,Values="*" Name=status,Values="available" Name=availability-zone,Values="$EC2_AVAIL_ZONE" | grep -o 'eni-[a-z0-9]*' | head -1`
              ENI_ID_LEN=`echo $ENI_ID | wc -m`
              if [ "$ENI_ID_LEN" == 1 ]; then
                echo no ENI ready yet
              else
                aws ec2 attach-network-interface --network-interface-id $ENI_ID --instance-id $SELF_INSTANCE_ID --device-index 1
                attached=$?
                echo result of trying to attach $attached
              fi
            done


              # Use the AWS CLI to see if the target ENI is already attached..
              #CURRENT_ATTACHMENT_ID=`aws ec2 describe-network-interfaces --filters "Name=tag:Name,Values=$ENI_NAME" "Name=subnet-id,Values=$SUBNET_ID" --query "NetworkInterfaces[0].[Attachment][0].[AttachmentId]" | grep -o 'eni-attach-[a-z0-9]*'`

              # ..and if it is, remove the attachment (the assumption here is that it's still attached to another instance that is being terminated).
              #if [ $CURRENT_ATTACHMENT_ID"X" != "X" ]; then
              #  echo Detaching $CURRENT_ATTACHMENT_ID;
              #  aws ec2 detach-network-interface --attachment-id $CURRENT_ATTACHMENT_ID
              #fi

              # Use the AWS CLI to get the id of the ENI to be attached
              #NETWORK_INTERFACE_ID=`aws ec2 describe-network-interfaces --filters "Name=status,Values=available" "Name=tag:Name,Values=$ENI_NAME" "Name=subnet-id,Values=$SUBNET_ID" --output json --query "NetworkInterfaces[0].NetworkInterfaceId" | grep -o 'eni-[a-z0-9]*'`

              # Attach the ENI (and display the attachment id)
              #echo Attaching $ENI_NAME to $ENI_INDEX  `aws ec2 attach-network-interface --network-interface-id $NETWORK_INTERFACE_ID --instance-id $SELF_INSTANCE_ID --device-index $ENI_INDEX`

              # get the brokerid from the tags for future use

              export BROKER_ID=`aws ec2 describe-network-interfaces --network-interface-ids $ENI_ID | grep -o 'broker[0-9]' | cut -c 7-`


          }

          attach_ebs() {
            # find correctly tagged ebs volume/s ie for this broker in this AZ and attach

            EC2_AVAIL_ZONE=`curl http://$AWS_METADATA_IP/latest/meta-data/placement/availability-zone`
            echo Running is AZ $EC2_AVAIL_ZONE

            # find a kafka EBS in the AZ we are running in that is not already attached
            # currenly assumes 1 EBS volume per instance

            # we need to wait until the volume is availble so if we don't get anything back then wait
            attached=255
            while [ "$attached" -gt "0" ]; do
               sleep 10
               echo waiting for volume
               VOLUME_ID=`aws ec2 describe-volumes --filters Name=tag-key,Values="kafkabrokerid" Name=tag-value,Values="broker$BROKER_ID" Name=status,Values="available" Name=availability-zone,Values="$EC2_AVAIL_ZONE" | grep -o 'vol-[a-z0-9]*'`
               VOLUME_ID_LEN=`echo $VOLUME_ID | wc -m`
               if [ "$VOLUME_ID_LEN" == 1 ]; then
                 echo no EBS volume ready yet
               else
                 aws ec2 attach-volume --volume-id $VOLUME_ID --instance-id $SELF_INSTANCE_ID --device /dev/sdh
                 attached=$?
                 echo result of trying to attach $attached
               fi
            done

            # wait for it to be attached otherwise it won't mount
             DATA_STATE="unknown"
                until [ "${!DATA_STATE}" == "attached" ]; do
                  DATA_STATE=$(aws ec2 describe-volumes \
                    --region $AWS_DEFAULT_REGION \
                    --filters \
                        Name=attachment.instance-id,Values=${!SELF_INSTANCE_ID} \
                        Name=attachment.device,Values=/dev/sdh \
                    --query Volumes[].Attachments[].State \
                    --output text)

                  sleep 5
                done

          }

          construct_zookeeper_config() {
          # get the ips
          # aws ec2 describe-network-interfaces --filters Name=tag-key,Values="kafka" Name=tag-value,Values="broker*" --profile dojo |grep "PrivateIpAddress" | grep -oE "\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" | sort -u

          # get list of zookeeper servers for zoo.cfg
          #  echo getting zookeeper servers `aws ec2 describe-network-interfaces --filters Name=tag-key,Values="kafka" Name=tag-value,Values="broker*" |grep "PrivateDnsName" | awk -F\" '{print $4}' | sed 's/$/:2888:3888/' | sort -u | awk '{printf "server.%s=\t%s\n",NR,$0}' | tr -d '[:blank:]'`
          #  aws ec2 describe-network-interfaces --filters Name=tag-key,Values="kafka" Name=tag-value,Values="broker*" |grep "PrivateDnsName" | awk -F\" '{print $4}' | sed 's/$/:2888:3888/' | sort -u | awk '{printf "server.%s=\t%s\n",NR,$0}' | tr -d '[:blank:]' > /tmp/zoo.cfg

          # get enis
          aws ec2 describe-network-interfaces --filters Name=tag-key,Values="kafkabrokerid" Name=tag-value,Values="broker*" |grep "NetworkInterfaceId" | awk -F\" '{print $4}' > /tmp/enis

          # get server id and hostname from enis

          while IFS= read -r var
          do
            # get broker number
            B_ID=`aws ec2 describe-network-interfaces --network-interface-ids $var | grep -o 'broker[0-9]' | cut -c 7-`
            DNS=`aws ec2 describe-network-interfaces --network-interface-ids $var | grep  'PrivateDnsName' | awk -F\" '{print $4}' | sort -u`
            IP=`aws ec2 describe-network-interfaces --network-interface-ids $var | grep  'PrivateIpAddress' | awk -F\" '{print $4}' | sort -u`
            HN=`aws ec2 describe-network-interfaces --network-interface-ids $var | grep  'PrivateDnsName' | awk -F\" '{print $4}' | cut -d"." -f1 |sort -u`
            echo server.$B_ID=$DNS:2888:3888 >> /tmp/zoo3.cfg
            echo $IP $HN $DNS >> /tmp/hosts
            echo $IP:2181 >> /tmp/connectstring
            ZC=$ZC,$IP:2181
          done < /tmp/enis
            ZC=$(echo $ZC | cut -c 2-)
            ZC=$(echo $ZC | tr -d ' ')


          # make the correct zk config

            sed -i '/^server/d' /etc/kafka/zookeeper.properties
            cat /etc/kafka/zookeeper.properties /tmp/zoo3.cfg > /tmp/zoo.cfg2
            rm /etc/kafka/zookeeper.properties
            cp /tmp/zoo.cfg2 /etc/kafka/zookeeper.properties

            # set /var/lib/zookeeper/myid to be the brokerid


            echo $BROKER_ID > /tmp/broker_id
            rm /var/lib/zookeeper/myid
            mv /tmp/broker_id /var/lib/zookeeper/myid


            cat /etc/hosts /tmp/hosts > /tmp/hosts2
            rm /etc/hosts
            cp /tmp/hosts2 /etc/hosts



          # remove test values from /etc/hosts
          sed -i '/^192/d' /etc/kafka/zookeeper.properties
          sed -i '/^192/d' /etc/hosts
          sed -i '/^zookeeper.connect/d' /etc/kafka/server.properties
          sed -i '/ssl/d' /etc/kafka/server.properties
          sed -i '/SSL/d' /etc/kafka/server.properties





          # add all zookeeper/kafka servers to /etc/hosts


          # put my broker id into the Kafka config

          sed -i '/broker.id/d' /etc/kafka/server.properties

          echo broker.id=$BROKER_ID > /tmp/kafkaprops
          echo zookeeper.connect=$ZC >> /tmp/kafkaprops
          cat /etc/kafka/server.properties /tmp/kafkaprops > /tmp/zc2
          rm /etc/kafka/server.properties
          cp /tmp/zc2 /etc/kafka/server.properties

          # use if trying to run on t2.micro
          #sed -i 's/1G/500M/g' /usr/bin/kafka-server-start

          }

          check_ebs_has_file_system()
          {
            NF=`sudo file -s /dev/xvdh | grep '/dev/xvdh: data' | wc -l`
            if [ $NF -eq 1 ]
            then
              echo making new ext4 file system
              mkfs -t ext4 /dev/xvdh
            fi
          }

          mount_ebs()
          {
            mkdir -p /data/1
            mount /dev/xvdh /data/1
          }

          chown -R ec2-user:ec2-user .
          attach_eni
          attach_ebs
          construct_zookeeper_config
          check_ebs_has_file_system
          mount_ebs
          zookeeper-server-start -daemon /etc/kafka/zookeeper.properties
          #wait for zookeeper
          sleep 20
          kafka-server-start -daemon /etc/kafka/server.properties

      ImageId:
        Fn::FindInMap:
        - AWSRegionArch2AMI
        - Ref: AWS::Region
        - Fn::FindInMap:
          - AWSInstanceType2Arch
          - Ref: InstanceType
          - Arch



  KafkaServerGroup:
      Type: AWS::AutoScaling::AutoScalingGroup
      Properties:

        AvailabilityZones:
          # Fn::GetAZs: !Ref AWS::Region only works here if we have subnets listed
          # below in VPCZoneIdentifier for all regions
          !GetAZs eu-west-1
        LaunchConfigurationName: !Ref KafkaLaunchConfiguration
        DesiredCapacity: 6
        MinSize: 6
        MaxSize: 6
        Cooldown: 300
#        HealthCheckType: ELB
#        HealthCheckGracePeriod: 30
#        VPCZoneIdentifier:
#          - 'Fn::ImportValue': !Sub 'vpc-${Branch}-0-${Environment}-${Stage}-${AWS::Region}-PrivateSubnet1'
        Tags:
        - Key: Name
          Value: 'Kafka Server'
          PropagateAtLaunch: true
        - Key: Application
          Value: 'kafka'
          PropagateAtLaunch: true
#      CreationPolicy:
#        ResourceSignal:
#          Timeout: PT15M
#      UpdatePolicy:
#        AutoScalingRollingUpdate:
#          PauseTime: PT15M
#          WaitOnResourceSignals: true



  KafkaInstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: '22'
        ToPort: '22'
        CidrIp:
          Ref: SSHLocation
#Outputs:

#  AZ:
#    Description: Availability Zone of the newly created EC2 instance
#    Value:
#      Fn::GetAtt:
#      - AvailabilityZone
#  PublicDNS:
#    Description: Public DNSName of the newly created EC2 instance
#    Value:
#      Fn::GetAtt:
#      - EC2Instance
#      - PublicDnsName
#  PublicIP:
#    Description: Public IP address of the newly created EC2 instance
#    Value:
#      Fn::GetAtt:
#      - EC2Instance
#      - PublicIp
